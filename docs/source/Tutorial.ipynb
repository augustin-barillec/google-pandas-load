{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Pre-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google_pandas_load import Loader\n",
    "from google_pandas_load import LoaderQuickSetup\n",
    "from google_pandas_load import LoadConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'dmp-y-tests'\n",
    "dataset_id = 'tmp'\n",
    "bucket_name = 'bucket_gpl'\n",
    "# gs_dir_path_in_bucket is the path in \n",
    "# the bucket of the directory that\n",
    "# will contain the data in Storage.  \n",
    "gs_dir_path_in_bucket = 'gpl_dir/subdir'\n",
    "local_dir_path = '/tmp/gpl_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(local_dir_path):\n",
    "    os.makedirs(local_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a loader\n",
    "\n",
    "Throughout this document, we call loader an instance of [google_pandas_load.Loader](Loader.rst) or of [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst). \n",
    "\n",
    "We emphasize that the second class is a daughter of the first one.\n",
    "\n",
    "The next two sections will be devoted to the creation of both classes through their main parameters which are data locations.\n",
    "\n",
    "### the low-level way\n",
    "\n",
    "To set up a loader the low-level way, use [google_pandas_load.Loader](Loader.rst).\n",
    "\n",
    "In the following code cell, credentials are inferred from the environment. Further information about how to authenticate to Google Cloud Platform with the [Google Cloud Client Libraries for Python](https://googleapis.github.io/google-cloud-python/latest/index.html) can be found \n",
    "[here](https://googleapis.github.io/google-cloud-python/latest/core/auth.html?highlight=defaults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bq_client to execute the load jobs' cloud parts, \n",
    "# which are the execution of queries, the extaction of BigQuery\n",
    "# tables to Storage and the load of tables to BigQuery from Storage. \n",
    "bq_client = bigquery.Client(\n",
    "    project=project_id, \n",
    "    credentials=None)\n",
    "\n",
    "# the dataset_ref pointing to the dataset to store the data \n",
    "# in BigQuery. \n",
    "dataset_ref = bigquery.dataset.DatasetReference(\n",
    "    project=project_id, \n",
    "    dataset_id=dataset_id)\n",
    "\n",
    "# the gs_client is used to instantiate a bucket. \n",
    "gs_client = storage.Client(\n",
    "    project=project_id, \n",
    "    credentials=None)\n",
    "# the bucket to store the data in Storage. \n",
    "bucket = storage.bucket.Bucket(\n",
    "    client=gs_client, \n",
    "    name=bucket_name)\n",
    "\n",
    "gpl = Loader(\n",
    "    bq_client=bq_client,\n",
    "    dataset_ref=dataset_ref,\n",
    "    bucket=bucket,\n",
    "    gs_dir_path_in_bucket=gs_dir_path_in_bucket,\n",
    "    local_dir_path=local_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the setup above, the bq_client, the dataset_ref and the gs_client share the same project_id. Furthermore, the bq_client and the gs_client share the same credentials. However neither the project_id nor the credentials are required to be the same.\n",
    "\n",
    "In order to be able to execute load jobs with all possible source and destination, the bq_client must have read and write access in both the dataset and the bucket.\n",
    "\n",
    "If one wants to use directly the bucket’s root directory to store the data loaded in Storage, one can set the gs_dir_path_in_bucket parameter to None.\n",
    "\n",
    "### the quick way\n",
    "\n",
    "To set up a loader quickly, use [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst).\n",
    "\n",
    "The code behind the instantiation is essentially the same as in the previous cell.\n",
    "\n",
    "Contrary to the low-level way the bq_client, the dataset_ref and the gs_client share the same project_id. Moreover the bq_client and the gs_client share the same credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl_quick_setup = LoaderQuickSetup(\n",
    "    project_id=project_id, \n",
    "    dataset_id=dataset_id, \n",
    "    bucket_name=bucket_name, \n",
    "    gs_dir_path_in_bucket=gs_dir_path_in_bucket,\n",
    "    credentials=None,\n",
    "    local_dir_path=local_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x\n",
       "0  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = gpl.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='bq',\n",
    "    data_name='a0',\n",
    "    dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command has created the following table in BigQuery:\n",
    "\n",
    "![](a0_in_bq.png)\n",
    "\n",
    "Its id in BigQuery is project_id:dataset_id.a0, where project_id is the dataset's one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic loading mechanism\n",
    "\n",
    "### source and destination\n",
    "\n",
    "The source and destination parameters of [google_pandas_load.Loader.load()](Loader.rst#google_pandas_load.Loader.load) take one  of the following values: \n",
    "\n",
    "- 'query', \n",
    "- 'bq' \n",
    "- 'gs', \n",
    "- 'local'\n",
    "- 'dataframe'\n",
    "\n",
    "### Loading paths\n",
    "\n",
    "The downloading path is 'query'-> 'bq' -> 'gs' -> 'local' -> 'dataframe'.\n",
    "\n",
    "The uploading path goes in the opposite direction.\n",
    "\n",
    "### Load result in RAM\n",
    "\n",
    "- If destination = 'query', the following BigQuery standard SQL query is returned :  \n",
    "  \"select * from \\`project_id.dataset_id.data_name\\`\",  \n",
    "  where the project_id is the dataset's one. \n",
    "\n",
    "- If destination = ‘dataframe’, a pandas dataframe is returned.\n",
    "\n",
    "- Otherwise, None is returned.\n",
    "\n",
    "### In general, data is moved, not copied! \n",
    "\n",
    "Once the load job has been executed, the data usually does not exist anymore in the source and in any transitional locations.\n",
    "\n",
    "However two exceptions exist:  \n",
    "\n",
    "- When source = 'dataframe', the dataframe is not deleted in RAM.\n",
    "- When destination = ‘query’, the data is not deleted in BigQuery, so that it still exists somewhere. Indeed, in     this case, the load job returns a simple query (see the previous section) which represents the data but does not   contain it. \n",
    "\n",
    "Use the delete_in_bq, delete_in_gs and delete_in_local parameters from [google_pandas_load.Loader.load()](Loader.rst#google_pandas_load.Loader.load) to control the data deletion, during the execution of the load job.\n",
    "\n",
    "### In general, pre-existing data is deleted!\n",
    "\n",
    "Before new data is moved to any location, the loader will usually delete any prior data bearing the same name to prevent any conflict.\n",
    "\n",
    "There is one exception:\n",
    "\n",
    "- When destination = 'bq' and the write_disposition parameter from\n",
    "  [google_pandas_load.Loader.load()](Loader.rst#google_pandas_load.Loader.load) is set to 'WRITE_APPEND', new data   is appended to pre-existing one with the same name in the dataset. \n",
    "  \n",
    "## What is the data named data_name? \n",
    "\n",
    "- in BigQuery : the table in the dataset whose id is data_name.\n",
    "- in Storage : the blobs whose basename begins with data_name inside the bucket directory.\n",
    "- in local : the files whose basename begins with data_name inside the local folder.\n",
    "\n",
    "This definition is motivated by the fact that BigQuery splits a big table in several blobs when extracting it to Storage.\n",
    "\n",
    "## More examples\n",
    "\n",
    "\n",
    "### from query to gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='gs', \n",
    "    data_name='a0',\n",
    "    query='select 5 as y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from gs to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='gs', \n",
    "    destination='local', \n",
    "    data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from local to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpl.load(\n",
    "    source='local', \n",
    "    destination='dataframe', \n",
    "    data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from dataframe to gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='gs', \n",
    "    data_name='a0', \n",
    "    dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from gs to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = gpl.load(\n",
    "    source='gs', \n",
    "    destination='query', \n",
    "    data_name='a0', \n",
    "    bq_schema=[bigquery.SchemaField('y', 'INTEGER')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select * from `dmp-y-tests.tmp.a0`'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bq_schema can be inferred from the dataframe with  \n",
    "[google_pandas_load.LoadConfig.bq_schema_inferred_from_dataframe()](LoadConfig.rst#google_pandas_load.LoadConfig.bq_schema_inferred_from_dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('y', 'INTEGER', 'NULLABLE', None, ())]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_schema = LoadConfig.bq_schema_inferred_from_dataframe(df)\n",
    "bq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from \n",
    "(select 'Hello, ' as x from unnest(generate_array(1, 4000))) \n",
    "cross join \n",
    "(select 'World!' as y from unnest(generate_array(1, 4000)))\n",
    "\"\"\"\n",
    "\n",
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='gs',\n",
    "    data_name='a0',\n",
    "    query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list this data, [named](#What-is-the-data-named-data_name?) a0, in Storage:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Blob: bucket_gpl, gpl_dir/subdir/a0-000000000000.csv.gz>,\n",
       " <Blob: bucket_gpl, gpl_dir/subdir/a0-000000000001.csv.gz>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpl.list_blobs(data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to list the blob uris: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://bucket_gpl/gpl_dir/subdir/a0-000000000000.csv.gz',\n",
       " 'gs://bucket_gpl/gpl_dir/subdir/a0-000000000001.csv.gz']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpl.list_blob_uris(data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was big enough for BigQuery to split it into several files in Storage. \n",
    "\n",
    "Let us move this data into the local folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='gs', \n",
    "    destination='local',\n",
    "    data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list this data, [named](#What-is-the-data-named-data_name?) a0, in the local folder :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/gpl_directory/a0-000000000001.csv.gz',\n",
       " '/tmp/gpl_directory/a0-000000000000.csv.gz']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpl.list_local_file_paths(data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent BigQuery from splitting the data, set use_wildcard to False when creating the loader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gpl.exist_in_local(data_name='a1'))\n",
    "\n",
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='local',\n",
    "    data_name='a1',\n",
    "    query='select 2')\n",
    "\n",
    "print(gpl.exist_in_local(data_name='a1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete data\n",
    "\n",
    "### delete parameters\n",
    "\n",
    "Use the delete_in_bq, delete_in_gs and delete_in_local parameters to control data deletion in BigQuery, in Storage or in the local folder, during the execution of a load job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data={'x':[1]})\n",
    "\n",
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='bq',\n",
    "    data_name='a1',\n",
    "    dataframe=df, \n",
    "    delete_in_local=True, \n",
    "    delete_in_gs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the [default](#In-general,-data-is-moved,-not-copied!) value of these three parameters is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gpl.exist_in_local(data_name='a1'))\n",
    "print(gpl.exist_in_gs(data_name='a1'))\n",
    "print(gpl.exist_in_bq(data_name='a1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='gs',\n",
    "    data_name='a1',\n",
    "    query='select 2')\n",
    "\n",
    "print(gpl.exist_in_gs(data_name='a1'))\n",
    "gpl.delete_in_gs(data_name='a1')\n",
    "print(gpl.exist_in_gs(data_name='a1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data\n",
    "\n",
    "### cast data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x    y  z\n",
       "0  5  5.0  5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select 5 as x, 5 as y, 5 as z\n",
    "\"\"\"\n",
    "dtype = {\n",
    "    'x': str, \n",
    "    'y': float}\n",
    "\n",
    "df = gpl.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query=query, \n",
    "    dtype=dtype)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x     object\n",
       "y    float64\n",
       "z      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cast a column into the datetime.datetime type, use the parse_dates parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-14 14:32:30</td>\n",
       "      <td>2013-11-14 14:32:30.100121</td>\n",
       "      <td>2012-11-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    x                          y          z\n",
       "0 2012-11-14 14:32:30 2013-11-14 14:32:30.100121 2012-11-14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "cast('2012-11-14 14:32:30' as TIMESTAMP) as x, \n",
    "'2013-11-14 14:32:30.100121' as y,\n",
    "'2012-11-14' as z\n",
    "\"\"\"\n",
    "\n",
    "df = gpl.load(\n",
    "    source='query',\n",
    "    destination='dataframe',\n",
    "    query=query,\n",
    "    parse_dates=['x', 'y', 'z'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    datetime64[ns]\n",
       "y    datetime64[ns]\n",
       "z    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast data into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data={'x': [7, 8], 'y': ['a', 'b']})\n",
    "\n",
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='gs', \n",
    "    data_name='a0', \n",
    "    dataframe=df)\n",
    "\n",
    "\n",
    "bq_schema = [bigquery.SchemaField(name='x', field_type='FLOAT'),\n",
    "             bigquery.SchemaField(name='y', field_type='STRING')]\n",
    "\n",
    "gpl.load(\n",
    "    source='gs', \n",
    "    destination='bq', \n",
    "    data_name='a0', \n",
    "    bq_schema=bq_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('x', 'FLOAT', 'NULLABLE', None, ()),\n",
       " SchemaField('y', 'STRING', 'NULLABLE', None, ())]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref = dataset_ref.table(table_id='a0')\n",
    "table = bq_client.get_table(table_ref)\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If source = 'dataframe', the bq_schema argument is not required. In BigQuery, a column is given its type according to the following rule: \n",
    "\n",
    "- if its name is listed in the date_cols parameter, its type in BigQuery should be DATE.\n",
    "- elif  its name is listed in the timestamp_cols parameter, its type in BigQuery should be TIMESTAMP.\n",
    "- elif its pandas dtype is equal to numpy.bool, its type in BigQuery is BOOLEAN.\n",
    "- elif its pandas dtype has numpy.integer dtype as ancestor, its type in BigQuery is INTEGER.\n",
    "- elif its pandas dtype has numpy.floating dtype as ancestor, its type in BigQuery is FLOAT.\n",
    "- else its type in BigQuery is STRING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.strptime(\n",
    "    '2003-11-14 14:32:30.100121', \n",
    "    '%Y-%m-%d %H:%M:%S.%f')\n",
    "df = pandas.DataFrame(\n",
    "    data={\n",
    "        'w': [8.0], \n",
    "        'x': ['e'], \n",
    "        'y': ['2018-01-01'], \n",
    "        'z': [dt]})\n",
    "\n",
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='bq', \n",
    "    data_name='a0', \n",
    "    dataframe=df, \n",
    "    date_cols=['y'], \n",
    "    timestamp_cols=['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('w', 'FLOAT', 'NULLABLE', None, ()),\n",
       " SchemaField('x', 'STRING', 'NULLABLE', None, ()),\n",
       " SchemaField('y', 'DATE', 'NULLABLE', None, ()),\n",
       " SchemaField('z', 'TIMESTAMP', 'NULLABLE', None, ())]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref = dataset_ref.table(table_id='a0')\n",
    "table = bq_client.get_table(table_ref)\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoadConfig(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(data={'x': [3]})\n",
    "config2 = LoadConfig(\n",
    "    source='dataframe', \n",
    "    destination='local', \n",
    "    data_name='a0',\n",
    "    dataframe=df)\n",
    "\n",
    "load_results = gpl.mload(configs=[config1, config2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x\n",
       "0  1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(load_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitor a load job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xload_result = gpl.xload(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 11 as x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x\n",
       "0  11"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xload_result.load_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190404170236_690766_rand9273\n",
      "2\n",
      "Namespace(bq_to_gs=1, bq_to_query=None, dataframe_to_local=None, gs_to_bq=None, gs_to_local=0, local_to_dataframe=0, local_to_gs=None, query_to_bq=1)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(xload_result.data_name)\n",
    "print(xload_result.duration)\n",
    "print(xload_result.durations)\n",
    "print(xload_result.query_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitor a multi load job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoadConfig(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(data={'x': [3]})\n",
    "config2 = LoadConfig(\n",
    "    source='dataframe', \n",
    "    destination='local', \n",
    "    data_name='a0',\n",
    "    dataframe=df)\n",
    "\n",
    "xmload_result = gpl.xmload(configs=[config1, config2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   x\n",
       " 0  1, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmload_result.load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20190404170240_827732_rand9917', 'a0']\n",
      "3\n",
      "Namespace(bq_to_gs=2, bq_to_query=None, dataframe_to_local=0, gs_to_bq=None, gs_to_local=0, local_to_dataframe=0, local_to_gs=None, query_to_bq=1)\n",
      "0.0\n",
      "[0.0, None]\n"
     ]
    }
   ],
   "source": [
    "print(xmload_result.data_names)\n",
    "print(xmload_result.duration)\n",
    "print(xmload_result.durations)\n",
    "print(xmload_result.query_cost)\n",
    "print(xmload_result.query_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logger creating [google_pandas_load.Loader](Loader.rst)'s log records is named Loader and is controlled, as usual, by the application code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('Loader')\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(fmt='%(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(fmt=formatter)\n",
    "logger.addHandler(hdlr=ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loader - DEBUG - Starting query to bq...\n",
      "Loader - DEBUG - Ended query to bq [1s, 0.0$]\n",
      "Loader - DEBUG - Starting bq to gs...\n",
      "Loader - DEBUG - Ended bq to gs [1s]\n",
      "Loader - DEBUG - Starting gs to local...\n",
      "Loader - DEBUG - Ended gs to local [0s]\n",
      "Loader - DEBUG - Starting local to dataframe...\n",
      "Loader - DEBUG - Ended local to dataframe [0s]\n"
     ]
    }
   ],
   "source": [
    "df = gpl.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logger creating [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst)'s log records is named LoaderQuickSetup. Contrary to the logger Loader, it already has a built-in console handler. Therefore, without any logging set up, logging records are displayed in the console. This is convenient when running a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-04 17:02:49,955 - LoaderQuickSetup - DEBUG - Starting query to bq...\n",
      "2019-04-04 17:02:51,701 - LoaderQuickSetup - DEBUG - Ended query to bq [1s, 0.0$]\n",
      "2019-04-04 17:02:51,703 - LoaderQuickSetup - DEBUG - Starting bq to gs...\n",
      "2019-04-04 17:02:53,927 - LoaderQuickSetup - DEBUG - Ended bq to gs [2s]\n",
      "2019-04-04 17:02:53,929 - LoaderQuickSetup - DEBUG - Starting gs to local...\n",
      "2019-04-04 17:02:54,655 - LoaderQuickSetup - DEBUG - Ended gs to local [0s]\n",
      "2019-04-04 17:02:54,657 - LoaderQuickSetup - DEBUG - Starting local to dataframe...\n",
      "2019-04-04 17:02:54,665 - LoaderQuickSetup - DEBUG - Ended local to dataframe [0s]\n"
     ]
    }
   ],
   "source": [
    "df = gpl_quick_setup.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid duplicate log records in the console, the LoaderQuickSetup logger is by default set to not propagate its log records to its logger ancestors. \n",
    "\n",
    "Both [google_pandas_load.Loader](Loader.rst) and [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst) have a logger parmater. The default values are respectively the Loader logger and the LoaderQuickSetup logger. The parameters can, in both cases, be replaced by another logger. \n",
    "\n",
    "When using [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst), this is a convenient way to retake control of its log records. A use-case would be to stop displaying the logs in the console."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
