{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the imports used by this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google_pandas_load import Loader\n",
    "from google_pandas_load import LoaderQuickSetup\n",
    "from google_pandas_load import LoadConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these variables with the corresponding values of your own resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'dmp-y-tests'\n",
    "dataset_id = 'tmp'\n",
    "bucket_name = 'augustin-b-bucket'\n",
    "# gs_dir_path_in_bucket is the path in \n",
    "# the bucket of the directory that\n",
    "# will contain the data in Storage.  \n",
    "gs_dir_path_in_bucket = 'gpl_dir/subdir'\n",
    "local_dir_path = '/tmp/gpl_directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's not forget to create a local folder with path equals to local_dir_path otherwise the load jobs using it will crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(local_dir_path):\n",
    "    os.makedirs(local_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define, in this package context, a loader as an instance of [google_pandas_load.Loader](Loader.rst) or of [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst). \n",
    "\n",
    "The second class is the only daughter of the first class.\n",
    "\n",
    "Let's see how to create instances of both classes, with their main parameters : the locations where the data can be extracted from or moved to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the low-level way\n",
    "\n",
    "To set up a loader the low-level way, use [google_pandas_load.Loader](Loader.rst).\n",
    "\n",
    "In the following code cell, the credentials are inferred from the environment (See [here](https://googleapis.github.io/google-cloud-python/latest/core/auth.html?highlight=defaults) for more informations about how to authenticate to Google Cloud Platform with the [Google Cloud Client Libraries for Python](https://googleapis.github.io/google-cloud-python/latest/index.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bq_client to execute the cloud parts of the load jobs, \n",
    "# which are the execution of queries, the extaction of BigQuery\n",
    "# tables to Storage and loading tables to BigQuery from Storage. \n",
    "bq_client = bigquery.Client(\n",
    "    project=project_id, \n",
    "    credentials=None)\n",
    "\n",
    "# the dataset_ref pointing to the dataset to store the data \n",
    "# in BigQuery. \n",
    "dataset_ref = bigquery.dataset.DatasetReference(\n",
    "    project=project_id, \n",
    "    dataset_id=dataset_id)\n",
    "\n",
    "# the gs_client is used to instantiate a bucket. \n",
    "gs_client = storage.Client(\n",
    "    project=project_id, \n",
    "    credentials=None)\n",
    "# the bucket to store the data in Storage. \n",
    "bucket = storage.bucket.Bucket(\n",
    "    client=gs_client, \n",
    "    name=bucket_name)\n",
    "\n",
    "gpl = Loader(\n",
    "    bq_client=bq_client,\n",
    "    dataset_ref=dataset_ref,\n",
    "    bucket=bucket,\n",
    "    gs_dir_path_in_bucket=gs_dir_path_in_bucket,\n",
    "    local_dir_path=local_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the setup above, the bq_client, the dataset_ref and the gs_client share the same project_id. Furthermore, the bq_client and the gs_client share the same credentials. Both of these argument sharings are not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, in order to be able to execute load jobs with all possible source and destination, the bq_client must have read and write access to data in the dataset and in the bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the parameter gs_dir_path_in_bucket to None if you want to use directly the root directory of the bucket to contain the data loaded in Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the quick way\n",
    "\n",
    "To set up a loader quickly, use [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst).\n",
    "\n",
    "The code behind the instanciation of a object of this class is essentially the code of the previous cell.  \n",
    "\n",
    "A limitation is that the bq_client, the dataset_ref and the gs_client share necessarily the same project_id (the one specified as an argument). Another one is that the bq_client and the gs_client share the same credentials (those specified as an argument).\n",
    "\n",
    "A drawback is that these objects, built internally during the creation of the instance, could be used in other modules which do not need a loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl_quick_setup = LoaderQuickSetup(\n",
    "    project_id=project_id, \n",
    "    dataset_id=dataset_id, \n",
    "    bucket_name=bucket_name, \n",
    "    gs_dir_path_in_bucket=gs_dir_path_in_bucket,\n",
    "    credentials=None,\n",
    "    local_dir_path=local_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x\n",
       "0  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = gpl.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='bq',\n",
    "    data_name='a0',\n",
    "    dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BigQuery, there is now the following table : \n",
    "\n",
    "![](a0_in_bq.png)\n",
    "\n",
    " It has this table id : project_id:dataset_id.a0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic loading mechanism\n",
    "\n",
    "### source and destination\n",
    "\n",
    "The paramaters source and destination of [google_pandas_load.Loader.load()](Loader.rst#google_pandas_load.Loader.load) can take the following values : \n",
    "\n",
    "- 'query', \n",
    "- 'bq' \n",
    "- 'gs', \n",
    "- 'local'\n",
    "- 'dataframe'\n",
    "\n",
    "### Loading paths\n",
    "\n",
    "The downloading path is 'query'-> 'bq' -> 'gs' -> 'local' -> 'dataframe'.\n",
    "\n",
    "The uploading path is the reverted one.\n",
    "\n",
    "### Load result in RAM\n",
    "\n",
    "- If destination = 'query', the following BigQuery standard SQL query:  \n",
    "  \"select * from \\`project_id.dataset_id.data_name\\`\",  \n",
    "  where the project_id is the one of the dataset. \n",
    "\n",
    "- If destination = 'dataframe', a pandas dataframe populated with the loaded data. \n",
    "\n",
    "- Otherwise, None.\n",
    "\n",
    "### In general, data is moved, not copied ! \n",
    "\n",
    "Thus, in general, once the load job has been executed, the data does not exist anymore in the source and in the transitional locations. \n",
    "\n",
    "There are two exceptions : \n",
    "\n",
    "- When source = 'dataframe', the dataframe is not deleted in RAM (a function cannot delete a global variable         without knowing its name). \n",
    "- When destination = 'query', the data is not deleted in BigQuery, so that the data still exists somewhere. Indeed   in this case, the load job returns a simple query (see paragraph above), which represents the data but does not   contain the data.  \n",
    "\n",
    "Use the parameters delete_in_bq, delete_in_gs and delete_in_local of [google_pandas_load.Loader.load()](Loader.rst#google_pandas_load.Loader.load) to control the deletion of the data, during the execution of the load job.\n",
    "\n",
    "### In general, pre-existing data is deleted !\n",
    "\n",
    "In general, before data moves to any location, data with the same name already existing in the location is deleted, to make a clean space for the new data to come. \n",
    "\n",
    "There is one exception : \n",
    "\n",
    "- When destination = 'bq' and the parameter \n",
    "  [google_pandas_load.Loader.load()](Loader.rst#google_pandas_load.Loader.load) is set to 'WRITE_APPEND', the data   is appended to pre-existing data with the same name in the dataset. The default value of this parameter is         'WRITE_TRUNCATE'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the data named data_name ? \n",
    "\n",
    "For a loader, the data named data_name is :\n",
    "\n",
    "- in BigQuery : the table in the dataset whose id is data_name\n",
    "- in Storage : the blobs which are inside the bucket directory and whose basename begins with data_name\n",
    "- in local : the files which are inside the local folder and whose basename begins with data_name\n",
    "\n",
    "This defintion is motivated by the fact that BigQuery splits a big table in several blobs when extracting it to Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from query to gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='gs', \n",
    "    data_name='a0',\n",
    "    query='select 5 as y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from gs to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='gs', \n",
    "    destination='local', \n",
    "    data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from local to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpl.load(\n",
    "    source='local', \n",
    "    destination='dataframe', \n",
    "    data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from dataframe to gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='gs', \n",
    "    data_name='a0', \n",
    "    dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from gs to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = gpl.load(\n",
    "    source='gs', \n",
    "    destination='query', \n",
    "    data_name='a0', \n",
    "    bq_schema=[bigquery.SchemaField('y', 'INTEGER')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bq_schema can also be inferred from the dataframe with \n",
    "google_pandas_load.LoadConfig.bq_schema_inferred_from_dataframe :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('y', 'INTEGER', 'NULLABLE', None, ())]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_schema = LoadConfig.bq_schema_inferred_from_dataframe(df)\n",
    "bq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data in BigQuery and transfert it to Storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from \n",
    "(select 'Hello, ' as x from unnest(generate_array(1, 4000))) \n",
    "cross join \n",
    "(select 'World!' as y from unnest(generate_array(1, 4000)))\n",
    "\"\"\"\n",
    "\n",
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='gs',\n",
    "    data_name='a0',\n",
    "    query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list this data, [named](#What-is-the-data-named-data_name-?) a0, in Storage :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Blob: augustin-b-bucket, gpl_dir/subdir/a0-000000000000.csv.gz>,\n",
       " <Blob: augustin-b-bucket, gpl_dir/subdir/a0-000000000001.csv.gz>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpl.list_blobs(data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also list the blob uris : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://augustin-b-bucket/gpl_dir/subdir/a0-000000000000.csv.gz',\n",
       " 'gs://augustin-b-bucket/gpl_dir/subdir/a0-000000000001.csv.gz']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpl.list_blob_uris(data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was big enough for BigQuery to split it in several files in Storage. \n",
    "\n",
    "Let's move this data into the local folder : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl.load(\n",
    "    source='gs', \n",
    "    destination='local',\n",
    "    data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list this data, [named](#What-is-the-data-named-data_name-?) a0, in the local folder :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/gpl_directory/a0-000000000001.csv.gz',\n",
       " '/tmp/gpl_directory/a0-000000000000.csv.gz']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpl.list_local_file_paths(data_name='a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want BigQuery to not split the data, you can set use_wildcard to False when creating the loader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data existence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the exist methods to check the [existence](#What-is-the-data-named-data_name-?) of the data in BigQuery, in Storage or in the local folder. \n",
    "\n",
    "For instance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gpl.exist_in_local(data_name='a1'))\n",
    "\n",
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='local',\n",
    "    data_name='a1',\n",
    "    query='select 2')\n",
    "\n",
    "print(gpl.exist_in_local(data_name='a1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use delete parameters to control the deletion of the data in BigQuery, in Storage or in the local folder, during the execution of a load job. \n",
    "\n",
    "For instance, let's upload a dataframe into BigQuery, while keeping the data in Storage but not in the local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data={'x':[1]})\n",
    "\n",
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='bq',\n",
    "    data_name='a1',\n",
    "    dataframe=df, \n",
    "    delete_in_local=True, \n",
    "    delete_in_gs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that True is the [default](#In-general,-data-is-moved,-not-copied-!) value of the three parameters delete_in_bq, delete_in_gs and delete_in_local. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gpl.exist_in_local(data_name='a1'))\n",
    "print(gpl.exist_in_gs(data_name='a1'))\n",
    "print(gpl.exist_in_bq(data_name='a1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the delete methods to delete data in BigQuery, in Storage or in the local folder. \n",
    "\n",
    "For instance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "gpl.load(\n",
    "    source='query', \n",
    "    destination='gs',\n",
    "    data_name='a1',\n",
    "    query='select 2')\n",
    "\n",
    "print(gpl.exist_in_gs(data_name='a1'))\n",
    "gpl.delete_in_gs(data_name='a1')\n",
    "print(gpl.exist_in_gs(data_name='a1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when it moves to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data moves to pandas, use the parameter dtype to cast a column in one of the following python types : bool, int, float or str.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x    y  z\n",
       "0  5  5.0  5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select 5 as x, 5 as y, 5 as z\n",
    "\"\"\"\n",
    "dtype = {\n",
    "    'x': str, \n",
    "    'y': float}\n",
    "\n",
    "df = gpl.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query=query, \n",
    "    dtype=dtype)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x     object\n",
       "y    float64\n",
       "z      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cast a column into the datetime.datetime type, use the parameter parse_dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-14 14:32:30</td>\n",
       "      <td>2013-11-14 14:32:30.100121</td>\n",
       "      <td>2012-11-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    x                          y          z\n",
       "0 2012-11-14 14:32:30 2013-11-14 14:32:30.100121 2012-11-14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "cast('2012-11-14 14:32:30' as TIMESTAMP) as x, \n",
    "'2013-11-14 14:32:30.100121' as y,\n",
    "cast('2012-11-14' as DATE) as z\n",
    "\"\"\"\n",
    "\n",
    "df = gpl.load(\n",
    "    source='query',\n",
    "    destination='dataframe',\n",
    "    query=query,\n",
    "    parse_dates=['x', 'y', 'z'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    datetime64[ns]\n",
       "y    datetime64[ns]\n",
       "z    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when it moves to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data moves to BigQuery, you can specify the BigQuery types of the columns with the parameter bq_schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data={'x': [7, 8], 'y': ['a', 'b']})\n",
    "\n",
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='gs', \n",
    "    data_name='a0', \n",
    "    dataframe=df)\n",
    "\n",
    "\n",
    "bq_schema = [bigquery.SchemaField(name='x', field_type='FLOAT'),\n",
    "             bigquery.SchemaField(name='y', field_type='STRING')]\n",
    "\n",
    "gpl.load(\n",
    "    source='gs', \n",
    "    destination='bq', \n",
    "    data_name='a0', \n",
    "    bq_schema=bq_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the BigQuery table a0 has the bq_schema specified : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('x', 'FLOAT', 'NULLABLE', None, ()),\n",
       " SchemaField('y', 'STRING', 'NULLABLE', None, ())]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref = dataset_ref.table(table_id='a0')\n",
    "table = bq_client.get_table(table_ref=table_ref)\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If source = 'dataframe', bq_schema is not required. The pandas columns are given BigQuery types as follow and in this order of priority : \n",
    "\n",
    "- the columns whose name are in the list parameter date_cols are given the BigQuery type DATE. \n",
    "- the columns whose name are in the list parameter timestamp_cols are given the BigQuery type TIMESTAMP. \n",
    "- the columns with python type bool are given the BigQuery type BOOLEAN.\n",
    "- the columns with python type int are given the BigQuery type INTEGER. \n",
    "- the columns with python type float are given the BigQuery type FLOAT.\n",
    "- the other columns are given the BigQuery type STRING. \n",
    "\n",
    "Let's see an example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.strptime(\n",
    "    '2003-11-14 14:32:30.100121', \n",
    "    '%Y-%m-%d %H:%M:%S.%f')\n",
    "df = pandas.DataFrame(\n",
    "    data={\n",
    "        'w': [8.0], \n",
    "        'x': ['e'], \n",
    "        'y': ['2018-01-01'], \n",
    "        'z': [dt]})\n",
    "\n",
    "gpl.load(\n",
    "    source='dataframe', \n",
    "    destination='bq', \n",
    "    data_name='a0', \n",
    "    dataframe=df, \n",
    "    date_cols=['y'], \n",
    "    timestamp_cols=['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('w', 'FLOAT', 'NULLABLE', None, ()),\n",
       " SchemaField('x', 'STRING', 'NULLABLE', None, ()),\n",
       " SchemaField('y', 'DATE', 'NULLABLE', None, ()),\n",
       " SchemaField('z', 'TIMESTAMP', 'NULLABLE', None, ())]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref = dataset_ref.table(table_id='a0')\n",
    "table = bq_client.get_table(table_ref=table_ref)\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi load\n",
    "\n",
    "The method [google_pandas_load.Loader.mload()](Loader.rst#google_pandas_load.Loader.mload) is used to launch several load jobs at the same time. For each job, the user defines a load job config which is an instance of [google_pandas_load.LoadConfig](LoadConfig.rst). Then the user gives as input for the mload method, this list of load configs. \n",
    "\n",
    "A load config has the same parameters than the method google_pandas_load.Loader.load.\n",
    "\n",
    "Let's see an example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoadConfig(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(data={'x': [3]})\n",
    "config2 = LoadConfig(\n",
    "    source='dataframe', \n",
    "    destination='local', \n",
    "    data_name='a0',\n",
    "    dataframe=df)\n",
    "\n",
    "load_results = gpl.mload(configs=[config1, config2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x\n",
       "0  1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(load_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a load job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have extra informations about a load job with the method [google_pandas_load.Loader.xload()](Loader.rst#google_pandas_load.Loader.xload). In particular, monitoring informations. \n",
    "\n",
    "For instance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xload_result = gpl.xload(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 11 as x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x\n",
       "0  11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xload_result.load_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190322203436_427238_rand3108\n",
      "2\n",
      "Namespace(bq_to_gs=1, bq_to_query=None, dataframe_to_local=None, gs_to_bq=None, gs_to_local=0, local_to_dataframe=0, local_to_gs=None, query_to_bq=1)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(xload_result.data_name)\n",
    "print(xload_result.duration)\n",
    "print(xload_result.durations)\n",
    "print(xload_result.query_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a multi load job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have extra informations about a multi load job with the method [google_pandas_load.Loader.xmload()](Loader.rst#google_pandas_load.Loader.xmload). In particular, monitoring informations. \n",
    "\n",
    "For instance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoadConfig(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(data={'x': [3]})\n",
    "config2 = LoadConfig(\n",
    "    source='dataframe', \n",
    "    destination='local', \n",
    "    data_name='a0',\n",
    "    dataframe=df)\n",
    "\n",
    "xmload_result = gpl.xmload(configs=[config1, config2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   x\n",
       " 0  1, None]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmload_result.load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20190322203440_093909_rand14', 'a0']\n",
      "3\n",
      "Namespace(bq_to_gs=2, bq_to_query=None, dataframe_to_local=0, gs_to_bq=None, gs_to_local=0, local_to_dataframe=0, local_to_gs=None, query_to_bq=1)\n",
      "0.0\n",
      "[0.0, None]\n"
     ]
    }
   ],
   "source": [
    "print(xmload_result.data_names)\n",
    "print(xmload_result.duration)\n",
    "print(xmload_result.durations)\n",
    "print(xmload_result.query_cost)\n",
    "print(xmload_result.query_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logger creating the log records of [google_pandas_load.Loader](Loader.rst) is named Loader and is controlled, as usual, by the application code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('Loader')\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(fmt='%(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(fmt=formatter)\n",
    "logger.addHandler(hdlr=ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loader - DEBUG - Starting query to bq...\n",
      "Loader - DEBUG - Ended query to bq [1s, 0.0$]\n",
      "Loader - DEBUG - Starting bq to gs...\n",
      "Loader - DEBUG - Ended bq to gs [1s]\n",
      "Loader - DEBUG - Starting gs to local...\n",
      "Loader - DEBUG - Ended gs to local [0s]\n",
      "Loader - DEBUG - Starting local to dataframe...\n",
      "Loader - DEBUG - Ended local to dataframe [0s]\n"
     ]
    }
   ],
   "source": [
    "df = gpl.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logger creating the log records of [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst) is named LoaderQuickSetup. Contrary to the logger Loader, it has already a built-in console handler. Thus, without any logging set up, logging records are displayed in the console. For instance :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-22 20:34:48,200 - LoaderQuickSetup - DEBUG - Starting query to bq...\n",
      "2019-03-22 20:34:49,841 - LoaderQuickSetup - DEBUG - Ended query to bq [1s, 0.0$]\n",
      "2019-03-22 20:34:49,842 - LoaderQuickSetup - DEBUG - Starting bq to gs...\n",
      "2019-03-22 20:34:51,904 - LoaderQuickSetup - DEBUG - Ended bq to gs [2s]\n",
      "2019-03-22 20:34:51,906 - LoaderQuickSetup - DEBUG - Starting gs to local...\n",
      "2019-03-22 20:34:52,404 - LoaderQuickSetup - DEBUG - Ended gs to local [0s]\n",
      "2019-03-22 20:34:52,405 - LoaderQuickSetup - DEBUG - Starting local to dataframe...\n",
      "2019-03-22 20:34:52,412 - LoaderQuickSetup - DEBUG - Ended local to dataframe [0s]\n"
     ]
    }
   ],
   "source": [
    "df = gpl_quick_setup.load(\n",
    "    source='query', \n",
    "    destination='dataframe', \n",
    "    query='select 1 as x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is convenient when scripting for instance. \n",
    "\n",
    "In order to avoid duplicate log records in the console, the LoaderQuickSetup logger is set to not propagate its log records to its logger ancestors. \n",
    "\n",
    "[google_pandas_load.Loader](Loader.rst) and [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst) both have a parameter logger. The default values are respectively the Loader logger and the LoaderQuickSetup logger. In both cases, you can set up this parameter with another logger. \n",
    "\n",
    "This is mainly convenient when using [google_pandas_load.LoaderQuickSetup](LoaderQuickSetup.rst) to retake control of its log records (for instance to stop displaying them in the console). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
